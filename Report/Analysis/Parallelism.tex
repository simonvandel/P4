%mainfile: ../master.tex
\section{Parallelism}\label{sec:parallelism}

In the continued effort of trying to squeeze as much power out of computers as possible, the computer scientific community is at a point where increasing the clock speed of processors is no longer as viable a solution as it used to be. This has spawned an increased interest in increasing computational power in other ways, one being parallelism.

Parallelism is the act of dividing calculations into independently solvable parts, and then solving them on multiple processors before finally gathering the individual results and combining them. The main benefit of parallelising anything is to gain \emph{speed up} in the computation time of problems. This will be described in \cref{sup}.

With parallelism you gain \emph{speed up} through combining multiple processing units. This is seen in newer CPU's as multiple cores, but on a larger scale this principle can be used to create supercomputers, capable of performing immense calculations.

But even without a supercomputer a distributed network of multiple computers can provide with large amounts of parallel computing power. With this being a foreseeable future, we predict an increase in the access to, and need for, parallel systems.

\subsection{Speed Up}\label{sup}

For parallel computing, Gene Amdahl, an american computer scientist, defined a law for describing the theoretical maximum speedup using multiple processors. The maximum speedup a system can achieve by parallelisation is limited to at most 20×.
The law can be used to describe the speed up achievable by a giving system, by the percentage of parallelisable code, in this equation.

By this law maximising the percentage of parallelisable, the highest possible speed up achievable is increased, thereby improving the scaling of the solution on multiple processor.


\kanote{er de her kilder? de hang sammen med nedenstående}
\url{http://gribblelab.org/CBootcamp/A2_Parallel_Programming_in_C.html#sec-2-1}
\url{https://computing.llnl.gov/tutorials/parallel_comp/}

\subsection{Types of Tasks}\label{top}

Tasks within a problem can be depended on each other, in the sense that one task needs the output of a computation done by another task. This section will describe two types of problems relevant when doing parallel computations.

\subsubsection{Embarrassingly Parallel Problems}
A problem can be describe as being \emph{embarrassingly parallel} when the tasks within the problem is independent of each other, and can therefore be parallelised without any concern of the order in which the task can to be executed. They are trivially parallel in nature because of the independency. An example of this type of problem is incrementation of a large matrix, the individual cells in the matrix are completely independent from each other and can therefore be incremented without regard of other cells in the matrix.

\subsubsection{Serial Problems with Dependencies}
Although multiple similar simulations can be observed as being independent of each other, as utilised by the Monte Carlo method \url{http://en.wikipedia.org/wiki/Monte_Carlo_method}, most simulations do not satify the condition of being independent. Instead these are inherently sequential. They form a class of problems that cannot be split into independent sub-problems. In some cases it is not possible to gain speed up at all, by trying to parallelise a problem, which is not parallisesable. The only thing that a simulation designer can achieve in this case, is to add overhead to the computations. An example is calculating the fibonacci series by f(n) = f(n-1)+f(n-2), where f(n) is dependent on first finding the previous values of f.

\subsection{Parallel Data Access}
When trying to solve problems that are not embarrasingly parallel, with a parallel approach, some problems arise when multiple processes try to access the same memory. Among these the most common are race conditions and deadlocks.

\subsubsection{Race Conditions}
This problem arises when multiple processes want to modify the same data in memory or similar resource, and the outcome of the modification depends on the internal timing of processes. We call the resource "the shared resource" and the code which works with the shared resource "the critical region". An example of a race condition is two concurrent processes that want to raise the value of an integer by 1. In a normal modern day processor the process could be split into the three atomic operations\footnote{Atomic operations are operations which the hardware manufacturer ensures happens without disruptions and that cannot be split into smaller operations}:

\begin{itemize}
\item Copy the current value of the integer from main memory into register A
\item Calculate the value from register A, add 1 to it and place the result in register B
\item Take the new value from register B and override the integer in memory
\end{itemize}

Since we do not know when each process will try to access the memory, the value can either be raised by one, if both processes access it before the other overrides it, or raised by two, if one process finished before the other copy the value from memory. This is a well known problem and exists in many situations, where multiple processes work with non-atomic operations on the same memory. Some software solutions, that ensure only one instance of the critical region has permission to access the shared resource at the time, have been developed; especially Gary L. Peterson's algorithm, published in 1981, is used today. Other solutions have also been developed by creating atomic assembly instructions that can set a flag, thereby ensuring that only one critical region access the shared resource at the time.

\subsubsection{Deadlocks}

Deadlocks are a type of problem that occur when two or more processes are waiting for at least one of the others to finish, before it itself finishes, thereby never finishing. This is a common problem within concurrent programming. Edward G. Coffman described four conditions that must be present if a deadlock is to occur~\cite{Coffman:1971}:

\begin{description}
\item[Mutual Exclusion] Resources can not be shared simultaneously by two processes.
\item[Hold and Wait] A process is at some point holding one resource, while waiting for another.
\item[No Preemption] A resources can not be released externally from a process.
\item[Circular Wait] A process must be waiting for a resource which is held by another process, which is waiting for the first process to release a resource.
\end{description}

By knowing these four conditions, we can try prevent deadlocks by making sure at least one of the conditions is not present.
