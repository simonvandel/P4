\chapter{Message-Passing Interface Standard}
Message-Passing Interface Standard (MPI) the de facto standard for message passing when doing high performance computation. It was first released in 1994 as version 1.0. It were created in collaboration between 40 different organisations involving about 60 people. It was based in a preliminary proposal, known as MPI1, that was created by Jack Dongarra, Rolf Hempel, Tony Hey, and David Walker and finished in November 1992 and inspired from current practice, such as PVM, NX, Express, p4, etc. It has since been iterated upon, with the latest version, the MPI-3.0, comming out in 2012.

The reason for creating this standard is to ensure portability and ease-of-use. In a distributed memory communication environment high-level routines and abstractions are often build upon lower level message passing routines of which this standardisation ensures that these abstractions and routines are available on all systems. This also present vendors with few clearly defined base routines they need to implement efficiently and, if possible, provide hardware support for enhancing scalability.

MPI is designed to allow efficient communication, use in heterogeneous environments and possibility for thread-safety. The semantics of the interface itself should be independent from any language it is implemented in, however it does focus on providing convenient bindings for the C and Fortran languages. MPI also specifies how communication failures are dealt with in the underlying systems.

The standard is designed for message passing only and does not include specifications for explicit operations on shared-memory. Operations that goes beyond the current operating system support are not included, such as interrupt-driven receives, remote execution, or active messages, since the interface is not intended for kernel programming/embedded systems. Neither does it include support for debugging, explicit support for threads or task management or support for I/O functions.

(Source: http://www.mpi-forum.org/docs/mpi-1.0/mpi-10.ps chapter 1)

\section{Messagetypes}

\subsection{Point-to-point operations}
\subsubsection{Send}
Point-to-point communication is the the basic building block of message passing. Here one process sends data in form of a message to a single other process.
\subsubsection{Recieve}
When a process receives the message enqueues the message in a queue called its message box. Each message in the message box is processed sequentially by dequeuing and handling them one at the time.

(Source: http://www.mpi-forum.org/docs/mpi-1.0/mpi-10.ps chapter 3)

\subsection{Collective operations}
\subsubsection{Broadcast}
Broadcast is a one-to-many operation, where one one process has some specific data that it sends to many other processes. The data is therefore multiplied, as opposed to being divided.

\subsubsection{Scatter}
Scatter is a close relative to broadcast. It is also a one-to-many operation but here the data is divided into equally large chunks and is distributed among multiple processes (including the process originally containing all the data). This could for instance be sending all entries in a collection to different processes that individually process their part.

\subsubsection{Gather}
Gather is a many-to-one operation and is the inverse of scatter. Here data from many processes are sent to one of them. This operation often implies a hierarchy of the processes containing the data where the process highest in the hierarchy receives all the data (also from itself).

\subsubsection{Reduce}
Reduce is a many-to-one operation. Here one operation, called the reduction function, is done on data from multiple processes and the result is placed in one of them. As in gather a hierarchy is also custom and the process highest in the hierarchy receives the result of the reduction. All reduction functions must be both associative and commutative so results can be reduced asymmetrically as data is sent from processes.
(Source: http://www.mpi-forum.org/docs/mpi-1.0/mpi-10.ps chapter 4)

\subsection{System buffer}
Consider the following; When sending messages, what happens if the receiver is not ready to process them? To solve this problem, MPI dictates that an implementation must be able to store messages, however the way this is done is up to the implementation.

One way to do this is with a system buffer. In short terms a system buffer works as an inbox, and sometimes also an outbox, where messages are stored until they can be processed. A few things to note about this inbox, is that it is supposed to work behind the scenes, not manageable by the programmer. However, what the programmer do need to realize, is that this buffer is a finite resource, which will be exhausted if one is not cautious.
(Source: MPI\_tut1)

\subsection{Blocking and Non-blocking sends}
On messages there can be made two distinct groups, the blocking sends and the non-blocking sends. The straight forward approach is the non-blocking send, where the sender assumes or is certain that the receiver is ready to handle the message. These types of messages returns almost immediately, but there is no guarantee that the message was received, or how it was received.

On the other hand there is the blocking send. This only returns after it is safe to modify the application buffer again. The sent data could still be sitting in a buffer, but it is considered safe.

Generally blocking sends are used for operations, and non-blocking when you wish to overlap those with communication in order to improve performance.
(Source: MPI\_tut2)

\subsection{Order and Fairness}
When sending messages, the order in which they are sent and receive can matter a great deal. MPI gives a few guarantees as to how this can be expected to happen. One must note that these rules are not enforced if multiple threads are participating in the communication.

When using MPI, messages shall not overtake each other. If one task sends out two messages in succession, the message sent out first will be the first one to be received. Also, if a receiver have multiple requests looking for a matching message, the request that was made first, will get the first match.

MPI does not give any guarantees related to fairness. It is entirely possible to starve tasks. If this is not desired, the responsibility lies with the programmer.
(Source: MPI\_tut3)